{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Arxiv Metadata using Arxiv API\n",
    "\n",
    "In this tutorial, we'll scrape metadata from Arxiv using Arxiv API. Before starting let's have quick overview of Arxiv data and ways of accessing it.\n",
    "\n",
    "### About Arxiv\n",
    "\n",
    "Arxiv is an electronic repository for preprints. It's mostly used by quantitative fields such as computer science, math etc. Papers on Arxiv are not subject to peer-review. Arxiv which is owned and operated by Cornell University is currently hosting over 1.5 million papers. More information about Arxiv can be found in [here](https://arxiv.org/).\n",
    "\n",
    "### Arxiv Data Access\n",
    "\n",
    "There are several options to scrape data from Arxiv. You need to first determine what kind of data you need. There are two types of data one can scrape from Arxiv.\n",
    "\n",
    "1. Metadata\n",
    "2. Full-Text\n",
    "\n",
    "Metadata includes information about papers such as title, authors, publication and last update dates, abstract, category, Arxiv ID for the paper etc. Metadata for Arxiv papers are stored and retrieved in Atom format. More information about Atom can be found in [here](https://validator.w3.org/feed/docs/atom.html).\n",
    "\n",
    "Full-Text data includes paper contents as well, mostly in Tex/Latex format. In this tutorial we'll study scraping metadata. There are three ways in which one can access metadata:\n",
    "\n",
    "1. OAI-PMH: Good for bulk metadata access.\n",
    "2. Arxiv API: Good for real-time programmatic use.\n",
    "3. RSS Feeds: Best for accessing daily updates on Arxiv.\n",
    "\n",
    "### Arxiv API\n",
    "\n",
    "Scraping metadata using Arxiv API is pretty straigtforward. One first need to construct a query. A query to Arxiv API is a single URL. For instance, let's say we'd like to access the first 2 papers whose titles include the word \"graph\" in category of statistical theory stat.TH. For the available category types and other query constructing tips please refer to [Arxiv API User Manual](https://arxiv.org/help/api/user-manual#query_details). Note that some categories are cross-listed.\n",
    "\n",
    "Example Query:\n",
    "\n",
    "http://export.arxiv.org/api/query?search_query=ti:graph+AND+cat:stat.TH&start=0&max_results=2\n",
    "\n",
    "\n",
    "### Handy Libraries in Python\n",
    "\n",
    "We'll use `urllib` to query Arxiv API and `feedparser` to parse the Atom returned. Let's retrieve the feed to the above query and print metadata for every paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import urllib.request\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_arxiv(search_query, start, max_results = -1):\n",
    "    # accessing Arxiv API\n",
    "    base_url = 'http://export.arxiv.org/api/query?'\n",
    "    \n",
    "    # constructing our query\n",
    "    query = 'search_query=%s&start=%i%s' % (search_query,\n",
    "                                          start,\n",
    "                                          \"\" if max_results == -1 else (\"&max_results=%i\"% max_results))\n",
    "    \n",
    "    # perform a GET request using the base_url and query\n",
    "    response = urllib.request.urlopen(base_url+query).read()\n",
    "\n",
    "    # parse the response using feedparser\n",
    "    feed = feedparser.parse(response)\n",
    "\n",
    "    return feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    search_query = 'ti:graph+AND+cat:stat.TH'\n",
    "    start = 0\n",
    "    max_results = 2\n",
    "\n",
    "    # Querying the Arxiv API\n",
    "    feed = query_arxiv(search_query, start, max_results) \n",
    "    \n",
    "    # Print the feed information\n",
    "    print('Feed last updated: %s' % feed.feed.updated)\n",
    "    print('Total results for this query: %s' % feed.feed.opensearch_totalresults)\n",
    "    print('Max results for this query: %s\\n' % len(feed.entries))\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        print(\"Title: \", entry.title)\n",
    "        print(\"Authors: \")\n",
    "        for name in (author.name for author in entry.authors):\n",
    "            print(name)\n",
    "        print(\"Publication Date: \", entry.published)\n",
    "        print(\"Arxiv ID: \", entry.id, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed last updated: 2019-03-09T00:00:00-05:00\n",
      "Total results for this query: 227\n",
      "Max results for this query: 2\n",
      "\n",
      "Title:  Bayesian Graph Selection Consistency For Decomposable Graphs\n",
      "Authors: \n",
      "Yabo Niu\n",
      "Debdeep Pati\n",
      "Bani Mallick\n",
      "Publication Date:  2019-01-14T05:24:55Z\n",
      "Arxiv ID:  http://arxiv.org/abs/1901.04134v1 \n",
      "\n",
      "Title:  Graph selection with GGMselect\n",
      "Authors: \n",
      "Christophe Giraud\n",
      "Sylvie Huet\n",
      "Nicolas Verzelen\n",
      "Publication Date:  2009-07-03T12:40:37Z\n",
      "Arxiv ID:  http://arxiv.org/abs/0907.0619v2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Queries\n",
    "\n",
    "* \"LSTM\" in title or abstract\n",
    "\n",
    "http://export.arxiv.org/api/query?search_query=all:LSTM\n",
    "\n",
    "* \"graph\" in title but category shouldn't be stat.TH, we start the result number 2 based on our query and expect 4 papers in total. Note that Arxiv returns 2000 maximum per query so it's useful that the order of papers are the same for the same query every time.\n",
    "\n",
    "http://export.arxiv.org/api/query?search_query=ti:graph+ANDNOT+cat:stat.TH&start=2&max_results=4\n",
    "\n",
    "* Papers of either Larry Wasserman or Daphne Koller. Note that if you don't specify number of results, it'll return 10 by default. Moreover, %22 is used to texts with more than one words. \n",
    "\n",
    "http://export.arxiv.org/api/query?search_query=au:%22daphne+koller%22+OR+au:%22larry+wasserman%22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
