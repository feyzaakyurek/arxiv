{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from Disk and Calculate Collab Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df = spark.read.parquet(\"Data/authors-catstat.ML+OR+catstat.AP+OR+catstat.CO+OR+catstat.ME+OR+catstat.OT+OR+catstat.TH-total49904.parquet\")\n",
    "collab_df = spark.read.parquet(\"Data/collab-catstat.ML+OR+catstat.AP+OR+catstat.CO+OR+catstat.ME+OR+catstat.OT+OR+catstat.TH-total49904.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id = 2 and id = 90\n",
    "# author1 = 2\n",
    "# author2 = 3338\n",
    "# depth_max = 10\n",
    "\n",
    "def collab_dist(author1, author2, depth_max = 3):\n",
    "    \n",
    "    # BFS\n",
    "    fifo = queue.Queue()\n",
    "    fifo.put(author1)\n",
    "\n",
    "    # To track depth to stop search at max depth\n",
    "    depth = queue.Queue()\n",
    "    depth.put(0)\n",
    "\n",
    "    # To find depth and the path backwards\n",
    "    parents = {author1 : -1}\n",
    "    i = 0\n",
    "    while not fifo.empty():\n",
    "        a = fifo.get() ; d = depth.get()\n",
    "#         print(\"AuthorID:\", a)\n",
    "        if a == author2:\n",
    "            break\n",
    "\n",
    "        if d > depth_max:\n",
    "            print(\"Max depth of %i is reached.\" % d)\n",
    "            break\n",
    "\n",
    "        # \"src\" in collab item is equal to author1, look for the authors in \"dest\"\n",
    "        df_dest = collab_df.filter(collab_df.src == a).select(collab_df.columns[1])\n",
    "        for i in [int(row.dest) for row in df_dest.collect()]:\n",
    "#             print(\"Next author: %i\" % i)\n",
    "            if i not in parents: #if already visited, don't add the queue\n",
    "                fifo.put(i); depth.put(d + 1)\n",
    "                parents[i] = a\n",
    "\n",
    "        # \"dest\" in collab item is equal to author1, look for the authors in \"src\"\n",
    "        df_src = collab_df.filter(collab_df.dest == a).select(collab_df.columns[0])\n",
    "        for i in [int(row.src) for row in df_src.collect()]:\n",
    "#             print(\"Next author: %i\" % i)\n",
    "            if i not in parents:\n",
    "                fifo.put(i); depth.put(d + 1)\n",
    "                parents[i] = a\n",
    "        i = i + 1\n",
    "        if i == 1000: break\n",
    "    # Calculate the depth.\n",
    "    dist = 0\n",
    "    ancestry = [a]\n",
    "    while parents[a] > 0:\n",
    "        dist = dist + 1\n",
    "        a = parents[a]\n",
    "        ancestry.append(a)\n",
    "    return (dist, d, ancestry, parents)\n",
    "    \n",
    "\n",
    "# print(\"Parents: %s\" % parents)\n",
    "# print(\"Dist:\", dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth of 4 is reached.\n"
     ]
    }
   ],
   "source": [
    "dist, d, ancestry, parents = collab_dist(8742,352,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[352, 1260, 323]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist\n",
    "ancestry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alex 6896\n",
    "Ale 352\n",
    "Joel 15622\n",
    "Peter 1690\n",
    "Daren Wang 6212\n",
    "Kayvan 350\n",
    "Addison 23894\n",
    "Martin 323\n",
    "Peter - Larry - Ale - Kayvan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33526"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = author_df.where(author_df.name == \"Geoffrey Hinton\").select(\"id\")\n",
    "r.rdd.map(lambda x: x.id).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1260, name='Aaditya Ramdas')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_df.filter(author_df.id == 1260).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=6896, name='Alex Reinhart')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_df.filter(author_df.name == \"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3338"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collab_dest = collab_df.filter(collab_df.dest == 2).select(collab_df.columns[0]).collect()\n",
    "collab_dest[1].src\n",
    "[int(row.mvv) for row in mvv_list.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(src=33525, dest=33526, arxiv='1206.4635v1', title='Deep Mixtures of Factor Analysers'),\n",
       " Row(src=9619, dest=33526, arxiv='1206.4635v1', title='Deep Mixtures of Factor Analysers'),\n",
       " Row(src=44985, dest=33526, arxiv='1902.01889v1', title='Analyzing and Improving Representations with the Soft Nearest Neighbor\\n  Loss'),\n",
       " Row(src=39191, dest=33526, arxiv='1902.01889v1', title='Analyzing and Improving Representations with the Soft Nearest Neighbor\\n  Loss'),\n",
       " Row(src=33525, dest=33526, arxiv='1206.6445v1', title='Deep Lambertian Networks'),\n",
       " Row(src=9619, dest=33526, arxiv='1206.6445v1', title='Deep Lambertian Networks'),\n",
       " Row(src=33313, dest=33526, arxiv='1412.7449v3', title='Grammar as a Foreign Language'),\n",
       " Row(src=34682, dest=33526, arxiv='1412.7449v3', title='Grammar as a Foreign Language'),\n",
       " Row(src=48591, dest=33526, arxiv='1412.7449v3', title='Grammar as a Foreign Language'),\n",
       " Row(src=48592, dest=33526, arxiv='1412.7449v3', title='Grammar as a Foreign Language'),\n",
       " Row(src=33652, dest=33526, arxiv='1412.7449v3', title='Grammar as a Foreign Language'),\n",
       " Row(src=33526, dest=33313, arxiv='1503.02531v1', title='Distilling the Knowledge in a Neural Network'),\n",
       " Row(src=33526, dest=49526, arxiv='1503.02531v1', title='Distilling the Knowledge in a Neural Network'),\n",
       " Row(src=39177, dest=33526, arxiv='1610.06258v3', title='Using Fast Weights to Attend to the Recent Past'),\n",
       " Row(src=33526, dest=33371, arxiv='1610.06258v3', title='Using Fast Weights to Attend to the Recent Past'),\n",
       " Row(src=33526, dest=39372, arxiv='1610.06258v3', title='Using Fast Weights to Attend to the Recent Past'),\n",
       " Row(src=33526, dest=50118, arxiv='1610.06258v3', title='Using Fast Weights to Attend to the Recent Past'),\n",
       " Row(src=35038, dest=33526, arxiv='1701.06538v1', title='Outrageously Large Neural Networks: The Sparsely-Gated\\n  Mixture-of-Experts Layer'),\n",
       " Row(src=35557, dest=33526, arxiv='1701.06538v1', title='Outrageously Large Neural Networks: The Sparsely-Gated\\n  Mixture-of-Experts Layer'),\n",
       " Row(src=50500, dest=33526, arxiv='1701.06538v1', title='Outrageously Large Neural Networks: The Sparsely-Gated\\n  Mixture-of-Experts Layer'),\n",
       " Row(src=50501, dest=33526, arxiv='1701.06538v1', title='Outrageously Large Neural Networks: The Sparsely-Gated\\n  Mixture-of-Experts Layer'),\n",
       " Row(src=50052, dest=33526, arxiv='1701.06538v1', title='Outrageously Large Neural Networks: The Sparsely-Gated\\n  Mixture-of-Experts Layer'),\n",
       " Row(src=33526, dest=49526, arxiv='1701.06538v1', title='Outrageously Large Neural Networks: The Sparsely-Gated\\n  Mixture-of-Experts Layer'),\n",
       " Row(src=44985, dest=33526, arxiv='1711.09784v1', title='Distilling a Neural Network Into a Soft Decision Tree'),\n",
       " Row(src=44985, dest=33526, arxiv='1811.06969v1', title='DARCCC: Detecting Adversaries by Reconstruction from Class Conditional\\n  Capsules'),\n",
       " Row(src=56936, dest=33526, arxiv='1811.06969v1', title='DARCCC: Detecting Adversaries by Reconstruction from Class Conditional\\n  Capsules')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collab_df.filter((collab_df.src == 33526) | (collab_df.dest == 33526)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dest = collab_df.filter(collab_df.src == a).select(collab_df.columns[1])\n",
    "[int(row.src) for row in df_dest.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(src=1557, dest=1555, arxiv='1406.7536v1', title='Estimating the distribution of Galaxy Morphologies on a continuous space')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collab_df.filter(collab_df.src == 1557).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[352, 1557, 1690]\n"
     ]
    }
   ],
   "source": [
    "distance, ancestry = arxiv.dist(\"Peter Freeman\", \"Alessandro Rinaldo\", depth_max = 3)\n",
    "print(distance) # 2\n",
    "print(ancestry) # [352, 1557, 1690], where Peter is 1690, Larry 1557 and Ale 352."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
