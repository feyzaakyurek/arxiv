{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying data from Arxiv API\n",
    "\n",
    "Atom parsing code in this notebook is mostly taken from the sample code on Arxiv API [documentation](https://arxiv.org/help/api/examples/python_arXiv_parsing_example.txt).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import feedparser\n",
    "from itertools import combinations\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_arxiv(search_query, start, max_results):\n",
    "    base_url = 'http://export.arxiv.org/api/query?'\n",
    "\n",
    "    query = 'search_query=%s&start=%i&max_results=%i' % (search_query,\n",
    "                                                         start,\n",
    "                                                         max_results)\n",
    "    # perform a GET request using the base_url and query\n",
    "    response = urllib.request.urlopen(base_url+query).read()\n",
    "\n",
    "    # parse the response using feedparser\n",
    "    feed = feedparser.parse(response)\n",
    "\n",
    "    return feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_authors(feed):\n",
    "    author_list = []\n",
    "    for entry in feed.entries:\n",
    "        for name in (author.name for author in entry.authors):\n",
    "            # maybe consider case insensitive comparing\n",
    "            if name not in map(lambda t: t[1], author_list):\n",
    "                author_list.append((len(author_list),name))        \n",
    "    return author_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_collabs(feed):\n",
    "    collab_list = []\n",
    "    for entry in feed.entries:\n",
    "        title = entry.title\n",
    "        arxiv_id = entry.id.split('/abs/')[-1]\n",
    "        authors = (author.name for author in entry.authors)\n",
    "        for a1,a2 in combinations(authors,2):\n",
    "            collab_list.append((find_id(a1,author_list),find_id(a2,author_list),arxiv_id,title))     \n",
    "    return collab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_id(a, author_list):\n",
    "    return [y[1] for y in author_list].index(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(l, schema):\n",
    "    rdd = sc.parallelize(l)\n",
    "    return spark.createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Arxiv API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed last updated: 2019-02-10T00:00:00-05:00\n",
      "Total results for this query: 43\n",
      "Max results for this query: 5\n"
     ]
    }
   ],
   "source": [
    "# Query parameters\n",
    "search_query = 'au:%22daphne+koller%22'\n",
    "start = 0\n",
    "max_results = 5\n",
    "\n",
    "# Querying the Arxiv API\n",
    "feed = query_arxiv(search_query, start, max_results) \n",
    "print('Feed last updated: %s' % feed.feed.updated)\n",
    "print('Total results for this query: %s' % feed.feed.opensearch_totalresults)\n",
    "print('Max results for this query: %s' % len(feed.entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, name='M. Pawan Kumar'),\n",
       " Row(id=1, name='Daphne Koller'),\n",
       " Row(id=2, name='Marc Teyssier'),\n",
       " Row(id=3, name='Urszula Chajewska'),\n",
       " Row(id=4, name='Nir Friedman'),\n",
       " Row(id=5, name='Ron Parr')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a data frame for unique authors <AuthorID, AuthorName>\n",
    "author_list = read_authors(feed)\n",
    "schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ])\n",
    "author_df = create_df(author_list, schema)\n",
    "author_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(src=0, dest=1, arxiv='1205.2633v1', title='MAP Estimation of Semi-Metric MRFs via Hierarchical Graph Cuts'),\n",
       " Row(src=2, dest=1, arxiv='1207.1429v1', title='Ordering-Based Search: A Simple and Effective Algorithm for Learning\\n  Bayesian Networks'),\n",
       " Row(src=3, dest=1, arxiv='1301.3840v1', title='Utilities as Random Variables: Density Estimation and Structure\\n  Discovery'),\n",
       " Row(src=4, dest=1, arxiv='1301.3856v1', title='Being Bayesian about Network Structure'),\n",
       " Row(src=1, dest=5, arxiv='1301.3869v1', title='Policy Iteration for Factored MDPs')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a data frame for collaborations <Author1ID, Author2ID, PaperArxivID, PaperTitle>\n",
    "collab_list = read_collabs(feed)\n",
    "schema = StructType([\n",
    "        StructField(\"src\", IntegerType(), True),\n",
    "        StructField(\"dest\", IntegerType(), True),\n",
    "        StructField(\"arxiv\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True)\n",
    "    ])\n",
    "collab_df = create_df(collab_list, schema)\n",
    "collab_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Author  and Collab Dataframes to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df.write.mode('overwrite').parquet(\"Data/authors-%s.parquet\" % search_query)\n",
    "collab_df.write.mode('overwrite').parquet(\"Data/collab-%s.parquet\" % search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://feyza.wv.cc.cmu.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
